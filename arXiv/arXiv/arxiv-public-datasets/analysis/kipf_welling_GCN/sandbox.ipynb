{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try get the main working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1b0098d081ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/internal-citations.json.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakegraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/arxiv-public-datasets/arxiv_public_data/tests/intra_citation.py\u001b[0m in \u001b[0;36mmakegraph\u001b[0;34m(data, clean, directed)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmakegraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdirected\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/networkx/classes/digraph.py\u001b[0m in \u001b[0;36madd_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_succ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_succ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Put arXiv data into right form for the GCN \"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import arxiv_public_data.tests.intra_citation as ia \n",
    "import time, json, gzip\n",
    "import pickle as pkl\n",
    "from arxiv_public_data.oai_metadata import load_metadata\n",
    "\n",
    "\n",
    "                                #Auxiliary\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def clean_labels(labels):\n",
    "    \"\"\" Some labels have multiple listings\n",
    "        so I take the first one\n",
    "        \n",
    "        Input: list of strings\n",
    "    \"\"\"\n",
    "\n",
    "    for i,label in enumerate(labels):\n",
    "\n",
    "        #If multiple listings, take first\n",
    "        label = label[0].split()[0]\n",
    "\n",
    "        #Merge sub-classes\n",
    "        label = label[:label.find('.')]\n",
    "\n",
    "        labels[i] = label\n",
    "    return labels\n",
    "\n",
    "\n",
    "def labels2categorical(labels):\n",
    "    \"\"\" labels are strings -- have form\n",
    "        'hep-th' -- So need to covert to\n",
    "        categoricals\n",
    "    \"\"\"\n",
    "        \n",
    "    #Create mapping\n",
    "    classes = set(labels)\n",
    "    class_labels = {}\n",
    "    for i,x in enumerate(classes):\n",
    "        class_labels[x] = i\n",
    "    class_labels\n",
    "    \n",
    "    #change\n",
    "    labels_categorical = []\n",
    "    for label in labels:\n",
    "        vec = np.zeros(len(classes))\n",
    "        temp = class_labels[label]\n",
    "        vec[temp] = 1\n",
    "        labels_categorical.append(vec)\n",
    "    return np.array(labels_categorical)\n",
    "\n",
    "\n",
    "def load_titles(G, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/title-embedding-usel-2019-03-19.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    title_vecs = np.array(out)\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    indicies = []\n",
    "    all_nodes = list(G.nodes())\n",
    "    for i,node in enumerate(nodes_string):\n",
    "        index = all_nodes.index(node)\n",
    "        indicies.append(index)\n",
    "    return title_vecs[indicies]\n",
    "\n",
    "\n",
    "\n",
    "def load_abstracts(G, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/abstract-embedding-usel-2019-03-19.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    abstract_vecs = np.array(out)\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    indicies = []\n",
    "    all_nodes = list(G.nodes())\n",
    "    for i,node in enumerate(nodes_string):\n",
    "        index = all_nodes.index(node)\n",
    "        indicies.append(index)\n",
    "    return abstract_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_fulltext(G, nodes_string, dirname):\n",
    "    \n",
    "    #Load the full feature matrix\n",
    "    filename = dirname + '/fulltext-embedding-usel-2-headers-2019-04-05.pkl'\n",
    "    out = []\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                out.extend(pkl.load(f))\n",
    "            except EOFError as e:\n",
    "                break\n",
    "    fulltext_vecs = np.array(out)\n",
    "    \n",
    "    #Then select the subset corresponding to the sub-graph we're examining\n",
    "    indicies = []\n",
    "    all_nodes = list(G.nodes())\n",
    "    for i,node in enumerate(nodes_string):\n",
    "        index = all_nodes.index(node)\n",
    "        indicies.append(index)\n",
    "    return fulltext_vecs[indicies]\n",
    "\n",
    "\n",
    "def load_labels(nodes_string, dirname):\n",
    "    m = load_metadata( dirname + '/oai-arxiv-metadata-2019-03-01.json.gz')\n",
    "    labels = [x['categories'] for x in m if x['id'] in nodes_string]\n",
    "    labels_cl = clean_labels(labels)\n",
    "    labels_cat = labels2categorical(labels_cl)\n",
    "    return labels_cat\n",
    "\n",
    "\n",
    "def save_data(nodes_int, dirname,vector_type, vector_train, vector_test, vector, G_sub):\n",
    "    \"\"\" Saves data in format required by Kipfs and Welling\n",
    "    \n",
    "        nodes_int = list, list of nodes labeled by integers\n",
    "        dirname = string, where to save the data\n",
    "        vector_type = string, = 'title', 'abstract', 'full-text'\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    #Save vectors\n",
    "    dirname = 'data'\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.x'\n",
    "    pkl.dump(vector_train, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.tx'\n",
    "    pkl.dump(vector_test, open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.allx'\n",
    "    pkl.dump(vector[:cutoff2], open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.y'\n",
    "    pkl.dump(np.array(labels_train), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ty'\n",
    "    pkl.dump(np.array(labels_test), open(fname,'wb'))\n",
    "\n",
    "    fname = dirname + '/ind.arXiv-' + vector_type + '.ally'\n",
    "    pkl.dump(np.array(labels_cat[:cutoff2]), open(fname,'wb'))\n",
    "\n",
    "    test_nodes = nodes_int[cutoff2:]\n",
    "    with open(dirname + '/ind.arXiv-' + vector_type + '.test.index','wt') as f:\n",
    "        for node in test_nodes:\n",
    "            f.write(str(node))\n",
    "            f.write('\\n')\n",
    "            \n",
    "            \n",
    "    #Save graph\n",
    "    graph_dict = {}       #put into their format\n",
    "    for node in G_sub.nodes():\n",
    "        graph_dict[node] = [x for x in G_sub.neighbors(node)]\n",
    "    pkl.dump(graph_dict, open(dirname + '/ind.arXiv-' + vector_type + '.graph', 'wb')) \n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "                                #Main\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    N = 10**2  #size of subgraph to be used (using a subraph during testing phase)\n",
    "\n",
    "    #Graph\n",
    "    t1 = time.time()\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output'\n",
    "    fname = dirname + '/internal-citations.json.gz'\n",
    "    q = json.load(gzip.open(fname, 'rt', encoding='utf-8'))\n",
    "    G = ia.makegraph(q)\n",
    "\n",
    "    \n",
    "    #Select subgraph if specified\n",
    "    if N != 0:\n",
    "        comps = nx.weakly_connected_components(G)\n",
    "        biggest = max(comps, key=len)\n",
    "        G_cc = G.subgraph(biggest)\n",
    "        nodes = list(G_cc.nodes())[:N]\n",
    "        G_sub = G_cc.subgraph(nodes)\n",
    "    else:\n",
    "        G_sub = G\n",
    "\n",
    "    nodes_string = list(G_sub.nodes())  #nodes labeled by ints\n",
    "    G_sub = nx.convert_node_labels_to_integers(G_sub)\n",
    "    nodes_int = list(G_sub.nodes())     #nodes labeled in strings\n",
    "    t2 = time.time()\n",
    "    print('Loading graph took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "\n",
    "    #Load features\n",
    "    t1 = time.time()\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data/output/embeddings'\n",
    "    title_vecs = load_titles(G, nodes_string, dirname)\n",
    "    abstract_vecs = load_abstracts(G, nodes_string, dirname)\n",
    "    #fulltext_vecs = load_fulltext(G, nodes_string, dirname)\n",
    "    \n",
    "    #Load labels\n",
    "    dirname = '/home/kokeeffe/research/arxiv-public-datasets/arxiv-data'\n",
    "    labels_cat = load_labels(nodes_string, dirname)\n",
    "    t2 = time.time()\n",
    "    print( 'Loading features & labels took ' + str((t2-t1)/60.0) + ' mins')\n",
    "\n",
    "    #Split into test & train & ulabeled portion\n",
    "    #For now, I'll assume that nothing is unlabeled\n",
    "    #That means cutoff1 and cutoff2 are the same\n",
    "    cutoff1 = int(0.9*title_vecs.shape[0]) \n",
    "    cutoff2 = int(0.9*title_vecs.shape[0])\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    abstract_vec_train, abstract_vec_test = abstract_vecs[:cutoff1], abstract_vecs[cutoff2:]\n",
    "    #fulltext_vec_train, fulltext_vec_test = fulltext_vecs[:cutoff1], fulltext_vecs[cutoff2:]\n",
    "    labels_train, labels_test = labels_cat[:cutoff1], labels_cat[cutoff2:]\n",
    "\n",
    "    #Save data\n",
    "    dirname = 'data'\n",
    "    save_data(nodes_int, dirname, 'title', title_vec_train, title_vec_test, title_vecs, G_sub)\n",
    "    save_data(nodes_int, dirname, 'abstract', abstract_vec_train, abstract_vec_test, abstract_vecs, G_sub)\n",
    "\n",
    "    #Combine title and abstract vecs\n",
    "    title_vecs = np.concatenate((title_vecs, abstract_vecs), axis=1)\n",
    "    title_vec_train, title_vec_test = title_vecs[:cutoff1], title_vecs[cutoff2:]\n",
    "    save_data(nodes_int, dirname, 'title-abstract', title_vec_train, title_vec_test, title_vecs, G_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv_public_data.oai_metadata import load_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv_public_data.config as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kokeeffe/research/arxiv-public-datasets/arxiv_public_data/tests/kipf_welling_GCN/arxiv-data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.DIR_BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kokeeffe/research/arxiv-public-datasets/arxiv_public_data/tests/kipf_welling_GCN/arxiv-data/fulltext'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.DIR_FULLTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
